{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Digit_Classification_using_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPS2TJSa+WfOPoDLsg1ZiZt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rogersm92/Machine_Learning/blob/main/Digit_Classification_using_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZI7ycO8elfV"
      },
      "source": [
        "# Introduction\r\n",
        "Keras is an Open Source (MIT License / GNU) library written in Python that provides us powerful tools for Neural Networks.\r\n",
        "<br></br>\r\n",
        "In this tutorial I'll show how to build a **feedforward neural network** and train it to identify handwritten digits.\r\n",
        "<br></br>\r\n",
        "A **feedforward neural network** is an artificial neural network wherein **connections between the nodes do not form a cycle**. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykGhgOfytxq5"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1mLfMcAQrJ62ONPhLqh6394T5eVW9FKXG)\r\n",
        "\r\n",
        "Given an image of a handrwitten digit, our neural network will identify it.\r\n",
        "Each image in the MNIST dataset is **28x28px** and contains a centered, grayscale digit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UoNsKzauqzq",
        "outputId": "be76b1fe-7707-4dcd-812b-60a78b5715d7"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers\r\n",
        "\r\n",
        "mnist = tf.keras.datasets.mnist # The MNIST database is a large database of handwritten digits that is commonly used for training various image processing systems\r\n",
        "print(tf.__version__)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzkVt_0RwD33"
      },
      "source": [
        "# Model / data parameters\r\n",
        "num_classes = 10\r\n",
        "input_shape = (28, 28, 1)\r\n",
        "# the data, split between train and test sets\r\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() \r\n",
        "# A feature is one column of the data in your input set, also known as X. -> x_train = train_images, x_test = test_images\r\n",
        "# A label is the thing we're predicting, also known as Y. --> y_train = train_labels, y_test = test_labels"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo1vlGJW0UZO",
        "outputId": "511bb82d-6614-4696-eab6-7731e7fb5bca"
      },
      "source": [
        "# Shape testing\r\n",
        "# The shape of an array is the number of elements in each dimension.\r\n",
        "myvector = np.array([1,2,3])\r\n",
        "print(myvector.shape)\r\n",
        "mymatrix = np.array([[1,2,3], \r\n",
        "                     [4,5,6]])\r\n",
        "print(mymatrix.shape)\r\n",
        "mytensor = np.array([[[1,2,3], \r\n",
        "                      [4,5,6], \r\n",
        "                      [7,8,9]],\r\n",
        "                     \r\n",
        "                     [[10,11,12], \r\n",
        "                      [13,14,15], \r\n",
        "                      [16,17,18]],\r\n",
        "                     ])\r\n",
        "print(mytensor.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3,)\n",
            "(2, 3)\n",
            "(2, 3, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwUytfQ1xzBt",
        "outputId": "4b295e96-b4d6-422f-9ee8-360f14226623"
      },
      "source": [
        "# Scale images to the [0, 1] range\r\n",
        "x_train = x_train.astype(\"float32\") / 255\r\n",
        "x_test = x_test.astype(\"float32\") / 255\r\n",
        "# Make sure images have shape (28, 28, 1) --> 28px x 28px\r\n",
        "# NumPy arrays have an attribute called shape that returns a tuple with each index having the number of corresponding elements.\r\n",
        "x_train = np.expand_dims(x_train, -1)\r\n",
        "x_test = np.expand_dims(x_test, -1)\r\n",
        "print(\"x_train shape:\", x_train.shape)\r\n",
        "print(x_train.shape[0], \"train samples\")\r\n",
        "print(x_test.shape[0], \"test samples\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpjq7kfByIoq"
      },
      "source": [
        "# convert class vectors to binary class matrices\r\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\r\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_MlM0R700zk",
        "outputId": "4c8bb2f2-9fa6-464e-89c3-779823435e2d"
      },
      "source": [
        "# Building the model\r\n",
        "model = keras.Sequential(\r\n",
        "    # A sequential model is a linear stack of layers.\r\n",
        "    # Each layer has exactly one input tensor and one output tensor. \r\n",
        "    [\r\n",
        "        keras.Input(shape=input_shape),\r\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\r\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\r\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\r\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\r\n",
        "        layers.Flatten(),\r\n",
        "        layers.Dropout(0.5),\r\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\r\n",
        "    ]\r\n",
        ")\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MiB92-T0_l3",
        "outputId": "fc221f0f-f167-4dd7-d200-475f5025d4bb"
      },
      "source": [
        "# Training the model\r\n",
        "batch_size = 128 # The batch size defines the number of samples that will be propagated through the network.\r\n",
        "# For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100. \r\n",
        "# The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. \r\n",
        "# Next, it takes the second 100 samples (from 101st to 200th) and trains the network again. \r\n",
        "epochs = 15 # An epoch is a term used in machine learning and indicates the number of passes of the entire training dataset the machine learning algorithm has completed.\r\n",
        "# In this case, the model will check the entire dataset 15 times, and if it were to overfit, it would stop and not complete all epochs.\r\n",
        "# Overfitting occurs when your model learns too much from training data and isn't able to generalize the underlying information. Because of that, it can't make precise predictions and is only able to work with training data.\r\n",
        "\r\n",
        "# An epoch contains all samples while batch size contains limited samples:\r\n",
        "# - One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.\r\n",
        "# - Batch size: Total number of training examples present in a single batch.\r\n",
        "\r\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\r\n",
        "\r\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "422/422 [==============================] - 36s 85ms/step - loss: 0.7511 - accuracy: 0.7709 - val_loss: 0.0813 - val_accuracy: 0.9783\n",
            "Epoch 2/15\n",
            "422/422 [==============================] - 36s 85ms/step - loss: 0.1177 - accuracy: 0.9641 - val_loss: 0.0541 - val_accuracy: 0.9862\n",
            "Epoch 3/15\n",
            "422/422 [==============================] - 36s 84ms/step - loss: 0.0842 - accuracy: 0.9750 - val_loss: 0.0472 - val_accuracy: 0.9872\n",
            "Epoch 4/15\n",
            "422/422 [==============================] - 35s 84ms/step - loss: 0.0685 - accuracy: 0.9786 - val_loss: 0.0401 - val_accuracy: 0.9892\n",
            "Epoch 5/15\n",
            "422/422 [==============================] - 37s 87ms/step - loss: 0.0599 - accuracy: 0.9816 - val_loss: 0.0376 - val_accuracy: 0.9898\n",
            "Epoch 6/15\n",
            "422/422 [==============================] - 37s 87ms/step - loss: 0.0547 - accuracy: 0.9826 - val_loss: 0.0367 - val_accuracy: 0.9910\n",
            "Epoch 7/15\n",
            "422/422 [==============================] - 36s 85ms/step - loss: 0.0490 - accuracy: 0.9845 - val_loss: 0.0333 - val_accuracy: 0.9895\n",
            "Epoch 8/15\n",
            "422/422 [==============================] - 36s 84ms/step - loss: 0.0447 - accuracy: 0.9861 - val_loss: 0.0339 - val_accuracy: 0.9895\n",
            "Epoch 9/15\n",
            "422/422 [==============================] - 36s 85ms/step - loss: 0.0407 - accuracy: 0.9872 - val_loss: 0.0315 - val_accuracy: 0.9910\n",
            "Epoch 10/15\n",
            "422/422 [==============================] - 36s 84ms/step - loss: 0.0403 - accuracy: 0.9869 - val_loss: 0.0289 - val_accuracy: 0.9913\n",
            "Epoch 11/15\n",
            "422/422 [==============================] - 36s 84ms/step - loss: 0.0378 - accuracy: 0.9881 - val_loss: 0.0305 - val_accuracy: 0.9915\n",
            "Epoch 12/15\n",
            "422/422 [==============================] - 38s 89ms/step - loss: 0.0361 - accuracy: 0.9880 - val_loss: 0.0280 - val_accuracy: 0.9917\n",
            "Epoch 13/15\n",
            "422/422 [==============================] - 36s 85ms/step - loss: 0.0319 - accuracy: 0.9901 - val_loss: 0.0306 - val_accuracy: 0.9923\n",
            "Epoch 14/15\n",
            "422/422 [==============================] - 36s 85ms/step - loss: 0.0302 - accuracy: 0.9905 - val_loss: 0.0285 - val_accuracy: 0.9920\n",
            "Epoch 15/15\n",
            "422/422 [==============================] - 36s 84ms/step - loss: 0.0296 - accuracy: 0.9902 - val_loss: 0.0295 - val_accuracy: 0.9917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdcb6595390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9GUN6eb6A8C"
      },
      "source": [
        "As the number of epochs increases, more number of times the weight are changed in the neural network and the curve goes from underfitting to optimal to overfitting curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWv_8sbl1JnX",
        "outputId": "7fe290d2-3745-4676-d2f2-2a28aa4fed8f"
      },
      "source": [
        "# Evaluating the training model\r\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\r\n",
        "print(\"Test loss:\", score[0])\r\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.023777900263667107\n",
            "Test accuracy: 0.9919000267982483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "id": "5YHDF5iu69ni",
        "outputId": "0ca7e273-bac7-4ed0-eb24-c2fe3d81a1df"
      },
      "source": [
        "# Testing the model\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "def predict_this_model(quantity):\r\n",
        "    # Predict on the first images\r\n",
        "    predictions = model.predict(x_test[:quantity])\r\n",
        "\r\n",
        "    # Print our model's predictions.\r\n",
        "    print(np.argmax(predictions, axis=1)) \r\n",
        "\r\n",
        "    # preview the images \r\n",
        "    plt.figure(figsize=(10,10))\r\n",
        "    x, y = 10, 4\r\n",
        "    for i in range(quantity):  \r\n",
        "        plt.subplot(y, x, i+1)\r\n",
        "        plt.imshow(x_test[i].reshape((28,28)),interpolation='nearest')\r\n",
        "        plt.axis('off')\r\n",
        "        plt.subplots_adjust(wspace=0.1, hspace=0.1)\r\n",
        "        plt.show()\r\n",
        "        print(y_test[i]) \r\n",
        "\r\n",
        "\r\n",
        "predict_this_model(10)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7 2 1 0 4 1 4 9 5 9]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEEAAABBCAYAAACO98lFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADOUlEQVR4nO3bXYhUZRzH8c/szuzozFqppdGaLiVGa+mVvSDBBkmELF3okkF7V0Z1YV3URSAEhZEkYVmXdSG9oQaBVhBCRWQahWtkC/YiIW1ubda27brbNtPFsytNbNOsnHkxn9/NOec5c57/f37n+5znnP+cSRWLRee7muqdQCMomiCagGgCogmIJoB0uZ1rmrr/V/Pnu4VdqenaIwmiCYgmIJqAaAKiCYgmIJqAaAKiCYgm4D+eHWaiwXtuBIt7vgJ9AwvB+FgGtL2akTsxDAqHjyYVNhFFEiRIwiMPvwLW5U+Fhiv/8YFOjk+MgO0/3nzWcQ4NLAH5bReC9P5Pz7qvKUUSkCpXcp9JPeH39deDn1YEX+d+GQ49dXV4hG9Z8Yut17wR+p09CvaNtIK1ueFp+xwtjjs4lgeds/4o2bd0371g2cZPKk0x1hPKKbFrQn73wcllafsFf1t/7tJO8MTq9rDv/TCTbO1cOn1yowX5I/1g/gd7wLUtYbbJHc8kkPVknMR6qkATP5wE+T1h+edke3734L8ec/LuMPUubwmpPv3zVaD9pW9CnwnkFYeDGpMwU6WXXG7HoztAJtUMdm2/BczvP5BYnEiCBieh76E2q7JhVvtiPEyr846OJB4nkqBBSRhbuwp8tv4ZZMF9mzaB2R8dSjxeJEGDkvDdbeHctKay7vx2Dci90wuq8btgJEGDkdA0Zw7ouelDMFQ4bWDLFSA7VvmD0ozjVq3nc0gNRcKxx5aDvRe/AG4/tk72reoRMKWGMOHXu24AR+54Fnw9EWoHw08tktVf9fhxOKgzCem2y8CDm18H2VRIZ0NvD7jk7eoPBSIJqBMJqXQIu3LvCdDdGooqL/+2ACzcHM5NoUb5RBLU65qwMpTIHl+ws6T5+S3d4KLe5AomlSiSoMYkNHcsAxtfe7OkvePFB0D7zo9rmc4ZRRLUmIS+++eCrtxQSfui98bDSp3+gBJJUEMSTnddZ3/XtsmtXK3CVqSamfD96maL06VffurmKDMUhkO93iaPw0GdbpaeHOwAB25tB8X+z+uRxhlFEiT4ksa5oPiSRhmVJeF8USRBNAHRBEQTEE1ANAH8BefctmuqoPMbAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAC5UlEQVRYhe2YTUhUURTHf/P8mCdl44RiZWH4MWgmarrIIEIqAxcGUoRYqSQUYW0EF9JGNFdKEgUtokWQhRoIIQV+ZJQOmpJljNVYSVBSSs9MAj+er4U2Oalj9a5jQ/PfzJx7L+f/48yZcx/PoGkaniZptQH+Rl5od8kL7S55JLSvq8390uFVm4dNM3WGpfY8stIeCe2yPf5Eg+WpqLJGSNww1oTbjvXI1nwCuwIIvdghykoMtNIYzfPES454at4/4UXaVW6kbKS2aQ9qv12EnWe2h25opTGa9sRbjvjKaASWuydJt2WRbssCICdwCHtesF4rh3S1x/TeZFoTLgN+VCsW7h9JgQ+fsCjdSLIMQEVnPCXBfUybp0XwAjqhx8P8kZCoViy0Zcajvnnp2BsoTQKgZn0VYGTzPXGdqAs66LqVQ91HMShjTA8NOu0VZDQDsFYy6rFYVLqnh2p7tWBt8HwqJ4Iq5yKZoqGdBDb3o+o1m5Pw6TF6LJX245WYJBmTJGOd8KG3PAl1bEyYh3DokR0aJkl2xLltBQQ0dAn1EHYjAkw2hWONqQJkEqy5AMQWvRbWFj8kDNo3YitlUXWYJZmeCQgvm0VVFUWUxU8vUYkia9+T5D/bbdktp7A8fSwq9QL9n9c4gJKbSmnoAwByB/cRWzwgIu2S0g3tG7aJ3Wc7HZeI1Ra1In3s5Kk3QX/JFho23AEgre8wscUDwqfFr9Jd6Z7MC47vptMzK15lEDynp0JN+E2GLVhXh0fQJiYwGI34hMw+oqohQdiL/J3OaaqBmDMDy96eQqEb668tur7rSTYjH9dhDvlKZ3KNyxzbzhUSUWx1eUY39EFbDi3b612e6Ui66RR/0yaZ0mYAyHiWB8CX3tlfIOzR8s/duqEDDrwlrqIQbV6mwJjPCyoa9zAf7d0aACLqx6GrDwAzdqfP35HB1ftp78sagfJCu0teaHfJ5fT4V+WRlfZCu0teaHfJI6G/Azta1OMmZ0qdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB7ElEQVRYhe2ZvUscQRiHn3XX8yLHkZwYiHJICjm4i4fgRyQGQROFQGKXP0EEIXWqNBZWwUaxEa3ExgRyFlqkCVhYeUgwAbHJh5dCTKKGUxd3d1IEDgS1iO+MLO6vWhh45uHH8M6yaymlCFuqrlrgfxJJm0okbSqhlHYuWuyven5l8/B9sGCdtxbKpiNpUzEibbXlWCoV+f7qgQgvavq87HQk8fCp/SEzjIxI/877bHsudTOrIjzt0qq7lZWn4wysvBBjapf+lb3BHbuWxjfVYkzt0o9GVnlXvkniw6YYU5u0nctg5zKM3S7y00/g7+2LsS9897hMSv11lee1P03AkRhbW9MH2RMOsicArE+2irK1NO0+6aAwMAHA6G4bqbcfCQT5Wpre7nPIx+LkY3EWv7QQlMui/FBe41qOR/29HXz170A4hVvifHFp524TrzMLTO+nAUjNylzdp/aQBm4NN9BVA0PFXgDSbEhvIS8dpI8BONqLS6MrEZeeuj8HQOOyLY2uRHR6HD/r5GFcdrydFdGmvw0qaiyH0d0WEoU1AHR8gxBr2k4medm9BMD8cg/K81CeJ4U/FbGmA9fl82EDj0vtNI99wpcCn5HrfSMq12WzHWJ81doyhLTpSNpUrOjvlqFE0qYSSZtKKKX/Ai3ReIl7lsQTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAC+klEQVRYhe2ZW0gUURjHf+Ps6hpCRRlurKBCahl0E4mNyoqkp7KLUtFDVpDaW4RgqGFEhkFE9xYSoQuISYhCQlhS0MPAallpFyErNzILabW8rLPTw9pStrvozLS2sD+YhznnzPn/+Tjf983uCIqiEGpETLcBNYRNB4uw6WARkqYNgSY3RuRMWz28564V/M2FZKTDpoOF7qbFWTPpurGMRoedRoedpW0gLkrWVSMcaQB3ooVnmVdxKTIuRebEPDvdW+fqqhGw5E15s3gLibYuPbf0iW6Rfl9mZXbNIJXmR3/NxVj7+FBqZWhLhi5aukW6/eB5XIrsc65lyU1YAne+m6kayMZw365JS5dIG1vMGAXR59U26qbxRyxGQSQ35jNN122a9TRHeig7gzxzrTfxfrG4OR+A2OYoor7JFGdG8CznHAA9xVYsFY9Va2oyLaalcOKMjfTIUUAEPEeg5MF2Fha9BEB2OgFIeZOMtNlERtQwdwsqyTIVkXDSjjIyMmVdTcfDHWkYN+xh37tN3NqwkuR8Cdnp9BoGkDteU1jtib5ZjKZ1/1mE1CRVurpVj6O96TgPzGGsx+F3TULdF0p7V2rW0mz6V8K1L1eQO98EXiwIGCLc3mc+lqvTDMk2rikRXxXM8FubfdG9bQ63YyVciohLkZl/DNwqdDWZLlndMDmReAsDK+ZzJe+Sd0waMSGMjqnS1fXdwx8d5XG8yLrgva8bnMvlIzmYOiVV+/1z08YWMxXmuj/Gqh1WTA3qDIPW5iJ4KgGAc7enlJUfv8a66GHvGqMgjp970TumrPdfFieDJtOnanaQu/8sAA9PX/QmpWvCb/iJ7X0BrVpktZlOqvmCtMfTmgMhjZiwfVpLf2EcqW+7mHy98Y2mOi13vKbs8AHSmgoDriusyufrqn7cTzv/aO1q0ZyI0fUSyfWwZtchjHt7aUqrIev5TtzV8wBQBEh40qc5ur8Tkh1RCPQlIPy3mI6ETQeLgGf6fyUkIx02HSzCpoNFSJr+Ce5Q8ltEHXWSAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACoElEQVRYhe2YW0gUURzGfzOzuqIt6xW8J1QqmrAs4gVfSjMlIeyhgqDLS0RBPgaBD2EQ9FBQUGRhSahEYb0UQfUgiV21gqKkAi2Uoi3dbSvZtZnpYWPLB3eWmWFsaL+nA//vnO/HYWbO+Y+gqip2k7jUAHqUgLZKCWirZEtoR6xis7h5yb6Ht5UrwmI1W+50AtoqJaCtkmnQ8love9+81fQFt9Yhla00lGUa9LsWJ5nSN03fx7YwvmPGYk2BFpKSaWx8FpfX9TSFLSVjSOlu3XmmQAc3eTlZcJddN/doekMZKh0Z4wgul+48w9Bqg4dTR0/Q97WI8s7Xmv769S+MRhqHnj34g0LHT/r3tyHPzsb0OvJyuVA8jGgw1tDsL7vruVx1nouBKpLujGn6X3YVMa/K7Jxch/zJpzvXELTY/pl8h5OegVZNr1RZRl9TNyF1nvfHS1FDIf25eidKOTl0lt4AoPDIPU3/+L50qp0yZ/0VpA0+1BsLGIAWUlNoSQ1Q83hHXP7skhkA+ieq9UZG9X8d48qMn8M+L9tWjOLIy43pdSwvYsRzCRGRuQfZeiP/rKd3ohIMcmu6nGHPAB+uuxnurl9Q91dEmp5lJQHq8idRUAAQTOiFhFh/mDTbrZoqAofmuLa6l0zJuaA0GpIAkBGpTg4jCZHuqb28ESUY1ASL1W7p3mkAHj3HvQG2r+nAv2ohdNa5+9Hx9NVKxmp7AeIC1pIx6N+Shp6QNbR4fW7SBbWRsdrgQRiJ73K1mEyB1pRA9Og2CgxWQatEX0QzZMl3WkmJAPtk/Uf337IEuq/1DK/CCs09B0xZz5YnoiXPdNfERr6fLqB4UPtiFY+seRGbpkhjyrTlbPl42BI65t3jX5UtdzoBbZUS0FbJltC/AJ94ssDLyixqAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACA0lEQVRYhe2ZwUsUYRiHn5nddXGRJJENVDQoowUvBhskXRRUyCSRpJP9A0F0iiAIwS5dQzyo3SKCbiGIKGiQkOBiXpRS8aAJZqLFps7uzoyHlTFxXcR957PFeU7zzQy/92Hmm3e+YTTbtsk39LMWOA2etCo8aVXkpbQ/28FGvePM+uGI9UE77lheXmlPWhWetCqUSG89vMXw6leWn9eh+bM2rBPhurS/vIzuFwMAzD7qRSsszDnTdemfzVU0hZIA3Jh6gBWP55yZ+706Bj0UAqD58WdnX/D9RRBYv7smbdRFAHgZfgPAtpXgwrsvItmuSS+1+w6N78+3Aasi2a5Jt0RnnO3f1g7Jrkvo/7O0cSdKT3m/M15Jgf5pWizfle6xFg0cGrcOPhHNd0W6oHbT2Z5LbHP99S/RfO81DrB79yZT0bfO+FsyjPl9UbSGuPROqY+AdtDunsbapUvIdw+jbQtIz2WAioFAttNPhai079qV/anhYyheA0BgNCZZAhCWXqsPO1OjZ6wRgGomJUsAwtK7JekP6JiRIPJqBYCUZIF9RKXDDT8A+PinFnNdtjf/i5i0Fgxyryy93thIFGEbhlT0EeRanmnSN3cbgPHlq2KxmTjfb0Q7leLys79EJjqxYsVSsRkRfRDNhSUqOyQTM3O+p4dKNO/vliI8aVV40qrIS+k9YVB6ia4bn2oAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAC+klEQVRYhe2XW0gUYRTHf7OztmpKlKyu2upCty1Fu5GuS5aBPdiVyCiKgoySLvQWVEIR+hZSlJGBXYkexIrAoIsYEaURJoldsIcyK121m2ab7c70EAYSuzvOzm6t7f9p+L7v/M+PjzNnzgiyLBNq0v1tADUKQwdLYehgKSSh9d4283WFf60f3pKqBU97IXnTYehgKQwdLGkCrU81k9QQS1tFFmLaNEUxotHIp402BIMBwWAYWT41kMMMTAkculPDtAiJRb0m3K1tPmNEo5H195rIjrzCjpZtvxYftyrO6ddN6ycmM65mkIwxIhl12xm/xDcwwLNSC2tiHKw8vgf5cSvyCIDBT+iPdjPnLLcBmF7iUBQj2zJ5ubSSvJZCzKefq8qrGlqfaqZ7hROAuYd34XrT4TNGtmVScvEcAP21Jty9H9TlVhUFvDkaQ9u8s5Q4ZpJ8phW3gpi3C8diN0ik399EyrH7alOrh5ZlgR+ym8ZeC+I376Whi43lRdkMri4vRyKClMIWtWkBDbrHdetViu7k0d6XyGCV6Y/9zvkyBVnNXEs6AURgb17LeJS9sJ6kGjr+WBT1pyLJi3JSlVKPDgGp/M+hUIeAxK/1S30JxO3TI6nnBUDw9jfuazTVmxL4kmOhY7HMy2UnafgOG24WDzsz5fx3aqtPAzCrcSPJq5S1t1E3mvpV067OLqIvdzH1MhQUzwZgKg+HndFlWNEhUNqTTuruz7j8STjkqYGHV7UfEJGQuVmWq6iXK5Hf3cOTerbaAHiSXcEr1zeiugc18w4Y9EB+/+/n1c1biK9v0sw7YNCVcy4A8N49QNyRaE29A1LTHXtzsBsk7AaJ165oRA1vGQIEvX5dHRIyEjJFjzYBIMZNQJw+RRP/gHcPya3DsTOH2XXdtJeN0cQzYDU9pGe5Z5ByZdLubmbywa+KpkFf+v++iJ50Y/8Cnu5NBOBBoxXr0XdM6nyB2+nUxN+vgSmQGnUDU0hCey2Pf1UhedNh6GApDB0shST0T0uP48KDaSFUAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAC/0lEQVRYhe2YX0hTcRTHP/dOXaYLaoquPyQZM/8nZlYPKUWWRA8WZhREvVgPJVGN6qU3IR8KIco0IUJI0JZhbxmRBemEMIJcWUtogVarUctt5XZvD4pl0Gy7l2uDfZ9+/A6/cz4czv2d87uCLMtEm8S5BohEMWitFIPWSlEJHRfKuEWsnrP7sEfqFP5mi8pMx6C1UgxaK8WgdSlGXrcVsbQ/GUGvR9Dr1XQ/LdWgPxzZQG2fjRebWrmyrBddihFdilEt9zMUsrn8q3TmTFpPNLI6IQ5pam+0yQCA6VA6gdExNcJMSxVo++mFFCToZuzZim8AMNz3g51tx1lRP4jk96sRTnl56HLM3NvcCEDDp2xqHNtm2M3xCVzd14RoSlMaalqKoV1rjWTEzeews4zegkR8lV6KLxyl3ZNGuycNEYGN8yDH+pY4U7oazMqhg3qQkHnWnA+AND6O6fxjOsZK6BgrQUImKEu8/25A9n9XDAwq1LRh1ygAX7aOs+jar/2zy7unVpN5eTS4CrN7QGm43zwqkMdqAuBAjg2hJB9X7Xo+dmeRFy+TFy9jn5gAoKvyIqwrUBoOiNKOqLg80rtHGD7zA4txiFO37UhMvhtqHNsB8NWlUtX+gIMLnDjqRDL7lUZUIdOB0TFqLccYCfjRCSIiAua7tfgqvfgqvUhPhzh3fwciAg1rbiEWZiuGFkL9YQrnufWtupTPu734v+jJtjgIut3TNtFgwGc10pNrpci2nyU7n8/qL9RzS5WOCJDcaSO5c3Id/MMmeTx87cqDXGgosHLZVK6otWv2IaY2D1D6ZC8VieO8PJmhyJd2t4cUxHh+Pq6gD/ueSwjFuRG70vTKE3sHKb9uQUTAU+9DNBgi86My16xa2eKkzZPOw/ybBAozI/KhOXTA+Y6OqjIAXJbIRtWo7IhzAh20v6LmTQV3ilon55EwZ5I5y7S3SsbmX4w7Kwl3VlJYZ1VrLuEq6PpEi3kFC+kL+2xU1nTI2eN/VVRmOgatlWLQWikqoX8Cy7Dh/TZwjUkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADKklEQVRYhe2YXUhTYRiAn525XInOrcWs7BcsjUgqNNOLgtCoLOjfIEsIol/pouiHrupGCKKMyiKvSrzQMop+pILI0GpaUNnc8l/6N9Km5dyOpwtrVOqaZ6fVas/NOO/3nfM+fHt5z/cdlSRJBBrCnxaQQ1DaXwSl/UVASod4GkwVVv6xfnijp0g10FhArnRQ2l94rGlFE0WZ6I4Z5b7W2F5g3TuRyGe9pWuwdCGUPfLqWcGV7o/2tUm8X9jFnunXWRdx1R3Pbx/LsvAS9Cu17lj66JlePVNxaSE+jprtYZSlHQFghNqM0M8fukHXDGj7xL1BcenOCeHYFpwEhg44J69tIgVNCX3iOmq9yuGzdEj0aCy7ozGVq4govIfgkLA5u2lxRQIwJqSNrKfr+WAZjsksEVnegtTRga7NO0HFpdWROhKvNHDReImUym0AhF4zs2tRFmK1tXdOXAwGax2GHhsALl8SfkV29xC0WhzFOvYZnzD5whaMJdXusW/CAKLlOfSIvln+hKyVVuv11BychDXuBFUOiD1Qj/jxo6JinpAl/XJtHNalx7jUqSc/PRXxXZ3SXh6RVR72WZ8BONowD9HmX2GQKV2YchqA4innqD2chDh3hqJSv+L/eY0nhmpwSiJ6QUvN6uM4V4lMvbUJnVlLR7RERD0YH3e657dOC8N0+61ipaTy9LFmoJOL7VQCtvS8QSV64FCx41kGhnSbV/MVP7lM3vqI+es3UmAfSUmnAaf06z6cGCpxd3oBdYdmy0n5A7LKQ3K50NysojC2d3+cuyIDUaMieecDcqLMA94nIBAd/0qe6XcosmEKK74PwOX42eRkmvkkdTPzzmYAxp1R05r9icqEc0qkAhTe5Y0tdUAmDFMNwTInH4DMcalcHV/Kt0psfm0ghkaf8ija8jSVz0l6uOaH2NnxNxAQcEhOUquXE5vtewdRVLrHbidqexeLrUuocKipcKgBON9hJL5oB6FpjYht7T7nkdXyvOFNdjIA9oTPxO5vxdXUMqj7/7mPNb/tYGvKLe/9RZmN//cE5EoHpf2Fx+7xtxKQKx2U9hdBaX8RkNJfAKKk+8D5VK7TAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADYklEQVRYhe2XaWgUZxiAn5nZzjq7TTGJmGigWszlQnBxdT0hATG/JA2pAQ+o0LRijUdEjX/8oSDSrkcVL+hhaY2BWiRBUdB4VNRqTDyLxgNjjMYjKCbEZHfN7Ex/RNKozaqbL2Nj9/k35/vMyzvf+36SaZr0NeR3LRAJUWmriEpbRZ+UtoW7OFnOf2frYYXxu9TdtT6Z6ai0VUSlrSIqbRVh1+lIkd0uAolOAOpyJaZ6q2g3FY7u8DLoWDPm+cs9er9QaXOCm9pCKB33Ax5VefWGpWfwL3nG900utl7MJKWgBiMQeOs4QsrDmOimttTN5tItXMvcjkdVqPBrVPg1Rp+dTkrZ1wRNneIHo9AklRHabdZ7f6N+0ciI4knhdi5v0sZrS93s7JLZ6bcmU3X1E9IX1nR8UGsrAAmnPqJxwRBSt11lecIfHPcPIsf5hNyxnwKg37n7wnt7pY3LTic3143lSuZPeFSFqqBJ6q65PJ2ik/plNUZra6cwQEZMA4ZNpnqNh3hZI9fZhEy3XmGJuKabcjI4kr8WGQeH/Xa+mTuL5IOnCb10n2SzIacN48fyONb8+gsZaiPgQJFkMipnkNR40zppU4GA2ZGpFkPjwRgVf56X5JT7ADQH+gGQP+Qchf13UP1MZYLdABwAnAwYJK2SMIPBt44dcU3LMTH4d8dTkl5CgqLxgaQQMg0AgqaOXfr3fOiEyLo0jbjCEHptXbex37vRNOLyMFpasGe3MDshj5oVQ8n2/MX15oHcbhiAoobISbsEgC+x+oXnXEdnk7a4Af1hY8TSPV7yuuNemQuAC94SAOr0NnI3FZO04Qymrr/2+XDl0Stt/NbqcZwb/d3zIxWAqb5iBm/5ExH7N+HS95aO58BMH5rk6Dy38UkyiT9fwBAUQ+iP2J49ivJ5Pj62/SNcr7exZ9kkjLY2YXGEZrpuisLQ58L3Qx2SnxctxrGvUmQYcdJKfBzn8zYAdgCyTswDYFiZWGEQJK3ExlJUeZwPpQ7hbx8PJ+WrGwDC6rgrQmr6UU462Y72zuP9K7NeGZhE0ic7ohDpz5Yc6pw7kvfOwblbfB13RYj0CK0eRZKpCpq4fJG35zdFiHTRzgIAvtg+P+zkJopemz16SrjZI6z0f5X/7+phNVFpq4hKW8Xf6KIlBeKdpC0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIy1PG2cDBu-"
      },
      "source": [
        "Each array has 10 positions, resembling a true or false array. Considering arrays start from index 0, it's the same as: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9.\r\n",
        "<br></br>\r\n",
        "Therefore, we have to compare the handwritten digit to the index where 1 belongs in each case. In all 10 examples above, the model guessed correctly all predictions.\r\n"
      ]
    }
  ]
}